import sys
import os
import json
# å¼ºåˆ¶UTF-8ç¼–ç ï¼ˆé¿å…ä¸­æ–‡è·¯å¾„æŠ¥é”™ï¼‰
os.environ["PYTHONIOENCODING"] = "utf-8"

# ========== 1. ç²¾å‡†è®¡ç®—æ‰€æœ‰ç›®å½• ==========
# å½“å‰æ–‡ä»¶è·¯å¾„ï¼šfeature/server/app/services/llm_service.py
CURRENT_FILE = os.path.abspath(__file__)
# servicesç›®å½•ï¼šfeature/server/app/services/
SERVICES_DIR = os.path.dirname(CURRENT_FILE)
# appç›®å½•ï¼šfeature/server/app/
APP_DIR = os.path.dirname(SERVICES_DIR)
# serverç›®å½•ï¼šfeature/server/
SERVER_DIR = os.path.dirname(APP_DIR)
# featureç›®å½•ï¼šfeature/ï¼ˆllmå’Œserverçš„çˆ¶ç›®å½•ï¼‰
FEATURE_DIR = os.path.dirname(SERVER_DIR)

# æ·»åŠ æ ¸å¿ƒç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.extend([FEATURE_DIR, SERVER_DIR, APP_DIR])

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import ast

# ========== 2. æ­£ç¡®çš„æ¨¡å‹è·¯å¾„ ==========
# llm/åœ¨feature/ä¸‹ï¼Œä¸server/åŒçº§
MODEL_DIR = os.path.normpath(os.path.join(FEATURE_DIR, "llm/models/Qwen-1_8B-Chat"))

QWEN_EOS_TOKEN_ID = 151643

# å…¨å±€ç¼“å­˜
_tokenizer = None
_model = None

def load_local_qwen_model():
    """åŠ è½½æœ¬åœ°åƒé—®æ¨¡å‹ï¼ˆå…¼å®¹4.32.0ç‰ˆæœ¬ï¼‰"""
    global _tokenizer, _model
    if _tokenizer is None or _model is None:
        try:
            # æ‰“å°è·¯å¾„ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print("="*50)
            print(f"ğŸ” featureç›®å½•ï¼š{FEATURE_DIR}")
            print(f"ğŸ” æ¨¡å‹ç›®å½•ï¼š{MODEL_DIR}")
            print("="*50)
            
            # éªŒè¯æ¨¡å‹ç›®å½•å­˜åœ¨
            if not os.path.exists(MODEL_DIR):
                raise FileNotFoundError(
                    f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼\n"
                    f"æœŸæœ›è·¯å¾„ï¼š{MODEL_DIR}\n"
                    f"è¯·ç¡®è®¤llm/æ–‡ä»¶å¤¹åœ¨featureç›®å½•ä¸‹"
                )
            
            # ========== å…³é”®ä¿®å¤ï¼šç›´æ¥åŠ è½½ï¼ˆ4.32.0ç‰ˆæœ¬æ— éœ€æ‰‹åŠ¨é…ç½®ï¼‰ ==========
            # åŠ è½½åˆ†è¯å™¨ï¼ˆå¼ºåˆ¶æœ¬åœ°åŠ è½½ï¼‰
            _tokenizer = AutoTokenizer.from_pretrained(
                MODEL_DIR,
                trust_remote_code=True,
                local_files_only=True,  # æ ¸å¿ƒï¼šåªåŠ è½½æœ¬åœ°æ–‡ä»¶
                padding_side="left",
                truncation_side="left",
                use_fast=False
            )
            _tokenizer.eos_token_id = QWEN_EOS_TOKEN_ID
            _tokenizer.pad_token_id = QWEN_EOS_TOKEN_ID

            # åŠ è½½æ¨¡å‹ï¼ˆå…¼å®¹åƒé—®é…ç½®ï¼‰
            _model = AutoModelForCausalLM.from_pretrained(
                MODEL_DIR,
                trust_remote_code=True,
                device_map="cpu",  # å¼ºåˆ¶CPUè¿è¡Œ
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                load_in_8bit=False,
                local_files_only=True,  # æ ¸å¿ƒï¼šåªåŠ è½½æœ¬åœ°æ–‡ä»¶
                use_safetensors=True
            ).eval()

            print("âœ… åƒé—®æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            raise Exception(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
    return _tokenizer, _model

def fix_code_indentation(code: str) -> str:
    """ä¿®å¤ä»£ç ç¼©è¿›"""
    if not code:
        return ""
    try:
        tree = ast.parse(code)
        indent_level = 0
        indent_step = 4
        fixed_lines = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.For, ast.While, ast.FunctionDef, ast.ClassDef)):
                fixed_lines.append(" " * indent_level * indent_step + ast.unparse(node).split("\n")[0])
                indent_level += 1
            elif isinstance(node, ast.Pass):
                fixed_lines.append(" " * indent_level * indent_step + "pass")
            elif isinstance(node, ast.Expr):
                fixed_lines.append(" " * indent_level * indent_step + ast.unparse(node))
        return "\n".join(fixed_lines)
    except SyntaxError:
        return code

def generate_code_from_requirement(requirement: str) -> str:
    """ç”ŸæˆPythonä»£ç """
    try:
        tokenizer, model = load_local_qwen_model()

        # æ„å»ºPrompt
        prompt = f"""
ç”ŸæˆPythonä»£ç å®ç°ä»¥ä¸‹æ•°æ®åˆ†æéœ€æ±‚ï¼š{requirement}
è¦æ±‚ï¼š
1. ä¸¥æ ¼éµå®ˆPythonç¼©è¿›è§„èŒƒï¼ˆ4ä¸ªç©ºæ ¼ç¼©è¿›ï¼‰ï¼›
2. ä»…è¾“å‡ºå¯è¿è¡Œçš„Pythonä»£ç ï¼Œæ— ä»»ä½•è§£é‡Šã€æ³¨é‡Šã€markdownæ ‡è®°ï¼›
3. ä¼˜å…ˆä½¿ç”¨å†…ç½®åº“ï¼ˆmath/statistics/randomï¼‰ï¼›
4. ä»£ç åŒ…å«å®Œæ•´çš„è¾“å…¥ã€è®¡ç®—ã€æ‰“å°è¾“å‡ºæ­¥éª¤ã€‚
        """.strip()

        # ç¼–ç è¾“å…¥
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=False
        )

        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.2,
                top_p=0.9,
                do_sample=True,
                eos_token_id=QWEN_EOS_TOKEN_ID,
                pad_token_id=QWEN_EOS_TOKEN_ID
            )

        # è§£ç å¹¶æå–ä»£ç 
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        code = generated_text.replace(prompt, "").strip()
        if code.startswith("```python"):
            code = code.replace("```python", "").replace("```", "").strip()

        # ä¿®å¤ç¼©è¿›
        return fix_code_indentation(code)
    except Exception as e:
        raise Exception(f"ä»£ç ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")

# æµ‹è¯•ä»£ç ï¼ˆå•ç‹¬è¿è¡ŒéªŒè¯ï¼‰
if __name__ == "__main__":
    try:
        tokenizer, model = load_local_qwen_model()
        print("\nğŸ“ æµ‹è¯•ç”Ÿæˆä»£ç ï¼š")
        code = generate_code_from_requirement("è®¡ç®—1åˆ°100çš„ç´¯åŠ å’Œ")
        print(code)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼š{e}")